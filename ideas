1.
对于BERT模型，您可以尝试使用预训练的大型模型（如BERT-large或RoBERTa），以获得更好的性能。您还可以尝试微调BERT模型的不同层，以适应特定任务的需求。例如，您可以仅微调BERT模型的顶部几层（如仅微调最后一层或最后两层），而将其余层保持固定，以避免过度拟合。

在使用BERT模型时，您还可以调整许多超参数，例如批量大小，学习率，权重衰减等，以获得更好的性能。例如，使用较大的批量大小可以加速训练过程，而使用较小的学习率可以有助于模型更好地收敛。

除此之外，您还可以使用其他的预处理技术，如数据增强、标准化、正则化等来优化模型性能。还可以使用其他的损失函数，如Focal Loss、Margin Loss等来加强模型的鲁棒性和分类性能。

总之，在调整模型时，建议使用验证集来评估不同超参数和模型结构的性能，以选择最佳的模型。


2.
对于多分类任务，可以考虑使用拼接法将CLS特征作为一个额外的输入通道添加到卷积层的输出中。这样可以充分利用CLS特征中包含的全局信息，同时保留卷积层的局部信息，从而在提高分类性能的同时避免过拟合。具体而言，可以将卷积层的输出大小为[B, C, L]，其中B为批次大小，C为通道数，L为序列长度，CLS特征的大小为[B, C, 1]。然后，使用torch.cat()函数将这两个张量进行拼接，得到一个大小为[B, C, L+1]的张量，将其送入后续的全连接层进行分类。

另外，可以通过添加Dropout层来进一步减少过拟合风险，例如在卷积层输出后添加一个Dropout层，以一定的概率随机丢弃部分神经元。这样可以使得模型更加稳定，提高泛化性能。

3.
先是二分类，选出evidence, 再多分类，分类
所以要改前面的preprocess的labels， seg_id 在前半阶段（选evidence）用不到